{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Rsearch Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a machine translation project that aims at developing a classifier that can differentiate between a human and machine translation. The translation is from mandarin to english. The information the model is given to predict with, is the original sentence in mandarin, followed by a human translation of the sentence and then the candidate translation for the sentence in mandarin. The model's goal is to predict whether or not the candidate translation is generated by a machine or by a human. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is based on a research paper by Professor Thorsten Joachims in 1998 on using SVMs in natural language processing. This method involves, the tokenization of sentences and word lemmatization to come up with useful information about the sentence at hand. It makes use of the property of SVMs which allows it to not overfit even with extremely high dimensional feature vectors. I have used this property along with word tokenization and lemmatization on each of the three sentences provided (for each example) to transform the data along with the additional data provided which was the score for the quality of the translation. The model uses a hard margin SVM to learn on the dataset and has an accuracy of 78.1609% on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Users/sidharthvasudev/Desktop/NLP research test/Untitled.ipynb\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/train.txt\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/~$dependent Study with LIL Lab.docx\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/SVM-Joachims (1998).ipynb\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/Independent Study with LIL Lab.docx\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/test.txt\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/quicktask.pdf\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/.ipynb_checkpoints/SVM-Joachims (1998)-checkpoint.ipynb\n",
      "//Users/sidharthvasudev/Desktop/NLP research test/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('//Users/sidharthvasudev/Desktop/NLP research test'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"//Users/sidharthvasudev/Desktop/NLP research test/train.txt\", \"r\").read()\n",
    "train_sep = train.rsplit(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is shown below is the cleaning process for the dataset provided (there are some irregular spaces when reading the file and converting to an array. Those are cleaned up using the two steps provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "while i < 3504:\n",
    "    try:\n",
    "        train_sep.pop(i)\n",
    "        i = i + 5\n",
    "    except:\n",
    "        i = 3504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the Dataset\n",
    "modified = []\n",
    "x = 4\n",
    "while x <= 2920: \n",
    "    if train_sep[x] == \"H\" or train_sep[x] == \"M\":\n",
    "        x = x + 5\n",
    "    else: \n",
    "        y = x\n",
    "        a = True\n",
    "        while a:\n",
    "            if train_sep[y] == \"H\" or train_sep[y] == \"M\":\n",
    "                add = train_sep[x-4: y+1]\n",
    "                add.append((x,y))\n",
    "                modified.append(add)\n",
    "                x = y\n",
    "                a = False\n",
    "            else:\n",
    "                y = y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_sep)\n",
    "train_res = pd.DataFrame(df.values.reshape(584,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process conducted above for the train set is done on the test set. This could be further abstracted into a function, but was not done so, just to maintain separation for clarity. Further work can be done over here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"//Users/sidharthvasudev/Desktop/NLP research test/test.txt\", \"r\").read()\n",
    "test_sep = test.rsplit(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "while i < 1044:\n",
    "    try:\n",
    "        test_sep.pop(i)\n",
    "        i = i + 5\n",
    "    except:\n",
    "        i = 1044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_sep)\n",
    "test_res = pd.DataFrame(df.values.reshape(174,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this, we describe the different tags and parts of speech we pay attention to. This could be further expanded with more time. Next, we work on tokenization and word lemmatization to provide numerical inputs for our model. This results in a high dimensional dataset with the properties of a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['D'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    text = text.lower()\n",
    "    text_list = word_tokenize(text)\n",
    "    Final = []\n",
    "    wordlemmatization = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(text_list):\n",
    "        word_lem = wordlemmatization.lemmatize(word, tag_map[tag[0]])\n",
    "        Final.append(word_lem)\n",
    "    return str(Final)\n",
    "all_data = pd.concat([train_res, test_res], ignore_index=True)\n",
    "all_data[\"0n\"] = all_data[0].apply(tokens)\n",
    "all_data[\"1n\"] = all_data[1].apply(tokens)\n",
    "all_data[\"2n\"] = all_data[2].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = all_data.iloc[0:584,:]\n",
    "Train_X = Train[[\"0n\",\"1n\",\"2n\",3]]\n",
    "Train_Y = Train[4]\n",
    "Test = all_data.iloc[584:,:]\n",
    "Test_X = Test[[\"0n\",\"1n\",\"2n\",3]]\n",
    "Test_Y = Test[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are essenntially converting the labels to -1s and +1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Encoder.fit(Train_Y)\n",
    "Train_Y = Encoder.transform(Train_Y)\n",
    "Test_Y = Encoder.transform(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we take the transformed sentences and vectorize them using tokenization. They are fitted to the train set so that the Test data also takes the same form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_0 = TfidfVectorizer()\n",
    "Tfidf_0.fit(Train_X[\"0n\"])\n",
    "Train_X0 = Tfidf_0.transform(Train_X[\"0n\"]).toarray()\n",
    "Test_X0 = Tfidf_0.transform(Test_X[\"0n\"]).toarray()\n",
    "Tfidf_1 = TfidfVectorizer()\n",
    "Tfidf_1.fit(Train_X[\"1n\"])\n",
    "Train_X1 = Tfidf_1.transform(Train_X[\"1n\"]).toarray()\n",
    "Test_X1 = Tfidf_1.transform(Test_X[\"1n\"]).toarray()\n",
    "Tfidf_2 = TfidfVectorizer()\n",
    "Tfidf_2.fit(Train_X[\"2n\"])\n",
    "Train_X2 = Tfidf_2.transform(Train_X[\"2n\"]).toarray()\n",
    "Test_X2 = Tfidf_2.transform(Test_X[\"2n\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrX = np.concatenate((Train_X0, Train_X1, Train_X2,Train_X[3].values.reshape(584,1)), axis=1)\n",
    "TeX = np.concatenate((Test_X0, Test_X1, Test_X2,Test_X[3].values.reshape(174,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is where we train the model. I have kept it simple and used a hard margin SVM (this is why I use the high C value, to make the SVM work like a hard margin SVM.) I have not added any other features, because I believe it to be the most efficient and effective version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  78.16091954022988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(C=15)\n",
    "svclassifier.fit(TrX, Train_Y)\n",
    "y_pred = svclassifier.predict(TeX)\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_pred, Test_Y) * 100)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "y_predtr = svclassifier.predict(TrX)\n",
    "print(\"SVM Accuracy: \", accuracy_score(y_predtr, Train_Y) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
