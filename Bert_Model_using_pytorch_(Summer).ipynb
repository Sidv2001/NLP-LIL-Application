{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "Bert Model using pytorch (Summer).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidv2001/NLP-LIL-Application/blob/master/Bert_Model_using_pytorch_(Summer).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf8wc3kOBh4A",
        "outputId": "7c7b2d2b-214d-4ec7-a21a-f628913e36fc"
      },
      "source": [
        "!pip install datasets==1.0.1\n",
        "!pip install transformers==3.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (3.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.1) (1.15.0)\n",
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.1.95)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEYvlndRBCXA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3U6EuZ3BCXM"
      },
      "source": [
        "train = open(\"//train.txt\", \"r\").read()\n",
        "train_sep = train.rsplit(\"\\n\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMVBMn4vBCXM"
      },
      "source": [
        "test = open(\"//test.txt\", \"r\").read()\n",
        "test_sep = test.rsplit(\"\\n\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnHpQxykBCXN"
      },
      "source": [
        "train_set = pd.DataFrame(train_sep)\n",
        "train_res = pd.DataFrame(train_set.values.reshape(584,6))\n",
        "train_dataset, val_dataset = train_test_split(train_res.drop(columns=[5]), test_size=0.2)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "val_dataset = val_dataset.reset_index(drop=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Tj7ONsSBCXN"
      },
      "source": [
        "test_set = pd.DataFrame(test_sep)\n",
        "test_res = pd.DataFrame(test_set.values.reshape(174,6))\n",
        "test_dataset = test_res.drop(columns=[5])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR8WFHh8BCXO"
      },
      "source": [
        "#transforming the dataset\n",
        "def transform_label(label):\n",
        "    if label == \"H\":\n",
        "        return 0\n",
        "    elif label == \"M\":\n",
        "        return 1\n",
        "    else:\n",
        "        raise\n",
        "train_dataset[4] = train_dataset[4].apply(transform_label)\n",
        "val_dataset[4] = val_dataset[4].apply(transform_label)\n",
        "test_dataset[4] = test_dataset[4].apply(transform_label)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "D3r7TR67BCXO",
        "outputId": "cec4c848-786c-49c9-fa59-a11c9f934d4e"
      },
      "source": [
        "val_dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>布 斯 瑞 迪 和数 以千计 的 其他 灾民 一样 , 因为 海啸 而 失去 身份证 和 其...</td>\n",
              "      <td>busriadi , like thousands of other refugees , ...</td>\n",
              "      <td>like thousands of other victims , busriadi los...</td>\n",
              "      <td>0.6781</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>连续 六 年 减少 援外 预算 的 日本 也 迅速 以 行动 粉碎 了 国力 日 衰 的 说...</td>\n",
              "      <td>japan , which has been reducing its foreign ai...</td>\n",
              "      <td>japan , which has cut its foreign aid budget i...</td>\n",
              "      <td>0.6304</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>外务 省 发言人 说 , 小 泉 所 说 的 五 亿 美元 也 包括 这 三 千万 美元 .</td>\n",
              "      <td>a foreign ministry spokesman said that the amo...</td>\n",
              "      <td>a spokesman for the foreign ministry said that...</td>\n",
              "      <td>0.4194</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>联合国 秘书长 安南 委托 以前 哈佛 经济学家 沙 克 斯 为首 的 开发 专家 编撰 这...</td>\n",
              "      <td>un secretary general annan has commissioned fo...</td>\n",
              "      <td>un secretary general annan commissioned develo...</td>\n",
              "      <td>0.7114</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>成千上万 的 尤 申 科 支持 者 已 在 首都 基辅 聚集 两 周 , 他们 包围 政府 ...</td>\n",
              "      <td>tens of thousands of yushchenko's supporters h...</td>\n",
              "      <td>thousands of supporters in the capital gathere...</td>\n",
              "      <td>0.4522</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>数 千 信徒 赶 往 伯 利 恒 参加 平安 夜 弥 撒</td>\n",
              "      <td>thousands of worshippers head to bethlehem for...</td>\n",
              "      <td>thousands of believers to bethlehem to partici...</td>\n",
              "      <td>0.4167</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>南韩 大使馆 则 说 , 李 海 瓒 将 在 斯里兰卡 总理 陪同 下 , 访 视 西部 海...</td>\n",
              "      <td>the south korean embassy said in a statement t...</td>\n",
              "      <td>south korean embassy , said that li hai , refl...</td>\n",
              "      <td>0.5828</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>ZEW 指数 十一月 由 前 月 的 三十 一点 三 , 大 幅 跌 至 十三点 九 .</td>\n",
              "      <td>the zew index fell sharply to 13.9 in november...</td>\n",
              "      <td>the zew index plummeted to 13.9 in november fr...</td>\n",
              "      <td>0.8667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>欧盟 在 与 中国 总理 温 家宝 举行 峰 会 后 发布 的 联合声明 中 说 : \" 欧...</td>\n",
              "      <td>\" the eu side has confirmed its political will...</td>\n",
              "      <td>in a joint declaration released after the summ...</td>\n",
              "      <td>0.8611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>欧 元 区 本 周 经济 指标 将 显示 欧洲 经济 持续 疲 弱</td>\n",
              "      <td>this week's eurozone economic indicators to sh...</td>\n",
              "      <td>euro zone economic targets this week will demo...</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>117 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     0  ...  4\n",
              "0    布 斯 瑞 迪 和数 以千计 的 其他 灾民 一样 , 因为 海啸 而 失去 身份证 和 其...  ...  0\n",
              "1    连续 六 年 减少 援外 预算 的 日本 也 迅速 以 行动 粉碎 了 国力 日 衰 的 说...  ...  0\n",
              "2       外务 省 发言人 说 , 小 泉 所 说 的 五 亿 美元 也 包括 这 三 千万 美元 .  ...  0\n",
              "3    联合国 秘书长 安南 委托 以前 哈佛 经济学家 沙 克 斯 为首 的 开发 专家 编撰 这...  ...  0\n",
              "4    成千上万 的 尤 申 科 支持 者 已 在 首都 基辅 聚集 两 周 , 他们 包围 政府 ...  ...  1\n",
              "..                                                 ...  ... ..\n",
              "112                       数 千 信徒 赶 往 伯 利 恒 参加 平安 夜 弥 撒  ...  1\n",
              "113  南韩 大使馆 则 说 , 李 海 瓒 将 在 斯里兰卡 总理 陪同 下 , 访 视 西部 海...  ...  1\n",
              "114       ZEW 指数 十一月 由 前 月 的 三十 一点 三 , 大 幅 跌 至 十三点 九 .  ...  0\n",
              "115  欧盟 在 与 中国 总理 温 家宝 举行 峰 会 后 发布 的 联合声明 中 说 : \" 欧...  ...  0\n",
              "116                  欧 元 区 本 周 经济 指标 将 显示 欧洲 经济 持续 疲 弱  ...  1\n",
              "\n",
              "[117 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS9-yNNGBCXP"
      },
      "source": [
        "class TranslationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, token_length, EtoEmodel='albert-base-v2'):\n",
        "\n",
        "        self.data = data\n",
        "        self.tokenizer1 = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.tokenizer2 = AutoTokenizer.from_pretrained(EtoEmodel)\n",
        "        self.tokenlength = token_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence1 = str(self.data.loc[index, 0])\n",
        "        sentence2 = str(self.data.loc[index, 1])\n",
        "        sentence3 = str(self.data.loc[index, 2])\n",
        "\n",
        "        encoded_pair1 = self.tokenizer1(sentence1, sentence2, \n",
        "                                      padding='max_length',\n",
        "                                      truncation=True,\n",
        "                                      max_length=self.tokenlength,  \n",
        "                                      return_tensors='pt')\n",
        "        \n",
        "        encoded_pair2 = self.tokenizer2(sentence2, sentence3, \n",
        "                                      padding='max_length',\n",
        "                                      truncation=True,\n",
        "                                      max_length=self.tokenlength,  \n",
        "                                      return_tensors='pt')\n",
        "        \n",
        "        encoded_pair3 = self.tokenizer1(sentence1, sentence3, \n",
        "                                      padding='max_length',\n",
        "                                      truncation=True,\n",
        "                                      max_length=self.tokenlength,  \n",
        "                                      return_tensors='pt')\n",
        "        \n",
        "        token_ids1 = encoded_pair1['input_ids']\n",
        "        attn_masks1 = encoded_pair1['attention_mask']\n",
        "        token_type_ids1 = encoded_pair1['token_type_ids']\n",
        "        token_ids2 = encoded_pair2['input_ids']\n",
        "        attn_masks2 = encoded_pair2['attention_mask']\n",
        "        token_type_ids2 = encoded_pair2['token_type_ids']\n",
        "        token_ids3 = encoded_pair3['input_ids']\n",
        "        attn_masks3 = encoded_pair3['attention_mask']\n",
        "        token_type_ids3 = encoded_pair3['token_type_ids']\n",
        "        bleu_score = self.data.loc[index, 3]\n",
        "        label = self.data.loc[index, 4]\n",
        "        return token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, torch.Tensor([float(bleu_score)]), torch.Tensor([float(label)])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQhBNVYpBCXQ",
        "outputId": "6fb74cd5-5998-490e-af27-f0dadb6ac296"
      },
      "source": [
        "class MachineHumanTranslationClassifier(nn.Module):\n",
        "    def __init__(self, EtoEmodel=\"albert-base-v2\", EtoEhs=768, freeze_bert=False):\n",
        "        super(MachineHumanTranslationClassifier, self).__init__()\n",
        "        \n",
        "        self.bert_layer1 = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.bert_layer2 = AutoModel.from_pretrained(EtoEmodel)\n",
        "        self.bert_layer3 = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer1.parameters():\n",
        "                p.requires_grad = False\n",
        "            for p in self.bert_layer2.parameters():\n",
        "                p.requires_grad = False\n",
        "            for p in self.bert_layer3.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.hidden_layer1 = nn.Linear(768, 1)\n",
        "        self.hidden_layer2 = nn.Linear(EtoEhs, 1)\n",
        "        self.hidden_layer3 = nn.Linear(768, 1)\n",
        "        #Final layer converges the results of the 3 hidden layers, and merges it into one out put with the bleu score\n",
        "        self.final_layer = nn.Linear(4, 1)\n",
        "        self.dropout_reg = nn.Dropout(p=0.1)\n",
        "        self.hidden_act = torch.nn.ReLU()\n",
        "        self.final_activation = torch.nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "    @autocast()\n",
        "    def forward(self, token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score):\n",
        "        _, pooler_output1 = self.bert_layer1(token_ids1, attn_masks1, token_type_ids1, return_dict=False)\n",
        "        _, pooler_output2 = self.bert_layer2(token_ids2, attn_masks2, token_type_ids2, return_dict=False)\n",
        "        _, pooler_output3 = self.bert_layer3(token_ids3, attn_masks3, token_type_ids3, return_dict=False)\n",
        "        res1 = self.hidden_layer1(self.dropout_reg(pooler_output1))\n",
        "        res2 = self.hidden_layer2(self.dropout_reg(pooler_output2))\n",
        "        res3 = self.hidden_layer3(self.dropout_reg(pooler_output3))\n",
        "        res_fin = torch.cat([res1, res2, res3, torch.Tensor([[bleu_score]])])\n",
        "        final_act = self.hidden_act(torch.transpose(res_fin, 0, 1))\n",
        "        final_out = self.final_layer(final_act)\n",
        "        \n",
        "        return self.final_activation(final_out)\n",
        "        \n",
        "             \n",
        "        "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUumJ-vJBCXQ"
      },
      "source": [
        "\n",
        "def evaluate_loss(classifier, criterion, dataloader):\n",
        "    classifier.eval()\n",
        "\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "    dataset_size = len(dataloader)\n",
        "    with torch.no_grad():\n",
        "        for i in range(dataset_size):\n",
        "            token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score, label = dataloader[i]\n",
        "            result = classifier(token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score)\n",
        "            mean_loss += criterion(result.squeeze(-1), label.float()).item()\n",
        "            count += 1\n",
        "\n",
        "    return mean_loss / count\n",
        "\n",
        "def accuracy_score(classifier, criterion, dataloader):\n",
        "    classifier.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    count = 0\n",
        "    dataset_size = len(dataloader)\n",
        "    with torch.no_grad():\n",
        "        for i in range(dataset_size):\n",
        "            token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score, label = dataloader[i]\n",
        "            result = classifier(token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score)\n",
        "            if result.item() >= 0.5:\n",
        "                pred = 1\n",
        "            else:\n",
        "                pred = 0\n",
        "            if pred == int(label.item()):\n",
        "                total_correct += 1\n",
        "            count += 1\n",
        "    return total_correct / count\n",
        "\n",
        "def evaluate_loss_acc(classifier, criterion, dataloader):\n",
        "    classifier.eval()\n",
        "\n",
        "    mean_loss = 0\n",
        "    total_correct = 0\n",
        "    count = 0\n",
        "    dataset_size = len(dataloader)\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(dataset_size)):\n",
        "            token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score, label = dataloader[i]\n",
        "            result = classifier(token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score)\n",
        "            mean_loss += criterion(result.squeeze(-1), label.float()).item()\n",
        "            if result.item() >= 0.5:\n",
        "                pred = 1\n",
        "            else:\n",
        "                pred = 0\n",
        "            if pred == int(label.item()):\n",
        "                total_correct += 1\n",
        "            count += 1\n",
        "    return mean_loss / count, total_correct / count\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOT9UDr0BCXR"
      },
      "source": [
        "def train_bert_clf(model, opti, lr, lr_scheduler, train_loader, val_loader, epochs, mini_batch):\n",
        "    total_iter = len(train_loader)\n",
        "    check = total_iter // 5  # print the training loss 5 times per epoch\n",
        "    loss_fn = nn.BCELoss()\n",
        "    check_loss = 0.0\n",
        "    dataset_size = len(train_loader)\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for i in tqdm(range(dataset_size)):\n",
        "            token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score, label = train_loader[i]\n",
        "            with autocast():\n",
        "                result = model(token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2, token_ids3, attn_masks3, token_type_ids3, bleu_score)\n",
        "                loss = loss_fn(result.squeeze(-1), label.float())\n",
        "                loss = loss / mini_batch\n",
        "                loss.backward()\n",
        "                \n",
        "            if (i + 1) % mini_batch == 0:\n",
        "                opti.step()\n",
        "                lr_scheduler.step()\n",
        "                opti.zero_grad()\n",
        "                \n",
        "            check_loss += loss.item()\n",
        "            \n",
        "            if (i+1) % check == 0:\n",
        "                print(\"Iteration {} / {} of epoch {} loss is: {}\"\n",
        "                      .format(i+1, total_iter, ep + 1, (check_loss / check)))\n",
        "                check_loss = 0.0\n",
        "        \n",
        "    train_loss, train_acc = evaluate_loss_acc(model, loss_fn, train_loader)\n",
        "    val_loss, val_acc = evaluate_loss_acc(model, loss_fn, val_loader)\n",
        "    print(\"Train Loss: {}\".format(train_loss))\n",
        "    print(\"Train Accuracy: {}\".format(train_acc))\n",
        "    print(\"Validation Loss: {}\".format(val_loss))\n",
        "    print(\"Validation Accuracy: {}\".format(val_acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "    return train_loss, train_acc, val_loss, val_acc, model\n",
        "        \n",
        "        \n",
        "            \n",
        "            \n",
        "            \n",
        "                \n",
        "                "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPqByYGzBCXS"
      },
      "source": [
        "bert_model = \"albert-base-v2\"  \n",
        "freeze_bert = False\n",
        "token_length = 128\n",
        "mini_batch = 2 \n",
        "lr = 2e-5\n",
        "epochs = 4\n",
        "loss_fn = nn.BCELoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRvF9BToBCXS"
      },
      "source": [
        "train_loader = TranslationDataset(train_dataset, token_length, bert_model)\n",
        "val_loader = TranslationDataset(val_dataset, token_length, bert_model)\n",
        "test_dataset = TranslationDataset(test_dataset, token_length, bert_model)\n",
        "model = MachineHumanTranslationClassifier()\n",
        "opti = AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "total_steps = (len(train_loader) // mini_batch) * epochs \n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e-S_A7FBCXS",
        "outputId": "0c9815cc-edad-42be-9787-0d84e9afa008"
      },
      "source": [
        "train_losses, val_losses, train_accuracies, val_accuracies, model = train_bert_clf(model, opti, lr, lr_scheduler, train_loader, val_loader, epochs, mini_batch)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/467 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
            " 20%|█▉        | 93/467 [09:27<36:06,  5.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 93 / 467 of epoch 1 loss is: 0.31444803489151824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 186/467 [18:56<30:10,  6.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 186 / 467 of epoch 1 loss is: 0.31135692471458065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 279/467 [28:17<17:58,  5.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 279 / 467 of epoch 1 loss is: 0.2897777224900902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 372/467 [37:39<09:56,  6.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 372 / 467 of epoch 1 loss is: 0.27636370615613076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 465/467 [46:54<00:11,  5.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 465 / 467 of epoch 1 loss is: 0.2648846418146164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 467/467 [47:06<00:00,  6.05s/it]\n",
            " 20%|█▉        | 93/467 [09:11<35:31,  5.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 93 / 467 of epoch 2 loss is: 0.21232041396120543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 186/467 [18:33<29:00,  6.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 186 / 467 of epoch 2 loss is: 0.2234598145850243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 279/467 [27:52<18:06,  5.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 279 / 467 of epoch 2 loss is: 0.23067563571917113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 372/467 [37:13<09:58,  6.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 372 / 467 of epoch 2 loss is: 0.24865091011248608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 465/467 [46:36<00:11,  5.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 465 / 467 of epoch 2 loss is: 0.21095804337372062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 467/467 [46:48<00:00,  6.01s/it]\n",
            " 20%|█▉        | 93/467 [09:17<35:56,  5.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 93 / 467 of epoch 3 loss is: 0.18188822021087012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 186/467 [18:45<30:15,  6.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 186 / 467 of epoch 3 loss is: 0.1945588959641354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 279/467 [28:53<19:54,  6.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 279 / 467 of epoch 3 loss is: 0.20824040985235603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 372/467 [39:28<11:25,  7.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 372 / 467 of epoch 3 loss is: 0.18624564494577148\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 465/467 [50:25<00:13,  6.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 465 / 467 of epoch 3 loss is: 0.1825745945135432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 467/467 [50:40<00:00,  6.51s/it]\n",
            " 20%|█▉        | 93/467 [11:51<46:03,  7.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 93 / 467 of epoch 4 loss is: 0.13970046949082165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 186/467 [23:53<38:19,  8.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 186 / 467 of epoch 4 loss is: 0.17743328197668956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 279/467 [35:52<23:28,  7.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 279 / 467 of epoch 4 loss is: 0.1806336275992855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|███████▉  | 372/467 [48:13<12:55,  8.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 372 / 467 of epoch 4 loss is: 0.16907327243637654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 465/467 [59:59<00:14,  7.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 465 / 467 of epoch 4 loss is: 0.18393851980887432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 467/467 [1:00:14<00:00,  7.74s/it]\n",
            "100%|██████████| 467/467 [12:50<00:00,  1.65s/it]\n",
            "100%|██████████| 117/117 [03:15<00:00,  1.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 0.320592951492209\n",
            "Train Accuracy: 0.9914346895074947\n",
            "Validation Loss: 0.4257999953742211\n",
            "Validation Accuracy: 0.9487179487179487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QPunOLLGDT5",
        "outputId": "d22383cd-f3b5-4627-ae59-95203b050461"
      },
      "source": [
        "train_loss, train_acc, val_loss, val_acc = train_losses, val_losses, train_accuracies, val_accuracies\n",
        "train_loss, train_acc, val_loss, val_acc"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.320592951492209, 0.9914346895074947, 0.4257999953742211, 0.9487179487179487)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7-e-L94BCXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59e0080-f4a4-4ef0-e595-6f468f6143aa"
      },
      "source": [
        "evaluate_loss_acc(model, loss_fn, test_dataset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 174/174 [04:53<00:00,  1.69s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4880423700201443, 0.867816091954023)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vLkVptLBCXT"
      },
      "source": [
        "optimal_model = model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRG3V0eEBCXT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}